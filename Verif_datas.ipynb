{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf664b3-147f-4796-93bc-f6a8a4724442",
   "metadata": {},
   "source": [
    "# Verification of the diagnosis by the RIBEIRO PRE-TRAINED Model \n",
    "  \n",
    "- On this notebook we wiil look further into the stats of the RIBEIRO NN and confirm the results annonced by his team.  \n",
    "- We will work over a part of the giant dataset of more thant 2 millions 12-lead ECG.  \n",
    "- For that we take 30000 ECG that will construct our validation dataset. It represents aproximately 1.3% of the whole training dataset\n",
    "- We will create 7 datasets, one for each cardiac disease and the last one if the patient is healthy.  \n",
    "- Also, in order to understand why the prediction is not correct, a {disease}_prob dataset will be created and some of the ECG that causes problem to the machine will be printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ec975-2923-4289-bae7-cf03acd30e03",
   "metadata": {},
   "source": [
    "# I. Determine diseases datasets  \n",
    "  \n",
    "First we analyse the 1.5%_dataset to find what are the index of the abnormal-ECG, we categorise them into 7 lists.  \n",
    "Those lists will be limited to 2 thousands index, it means that our stats won't be perfect but they will approximate the real ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68730148-0341-43ed-852c-f8b9f596d1b2",
   "metadata": {},
   "source": [
    "  ### I.1. Open datas and take only 30000 samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd656b-4ec1-4752-b434-90647dc0b34e",
   "metadata": {},
   "source": [
    "Taking the annotated ECG in order to compare the prediction to the annotation at the end.  \n",
    "We want 1 or 2 thousands ECG of each diseases, then 30000 datas looks alright to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1752fd7c-6cca-4c12-8850-5ec307336843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40635it [00:07, 5340.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du dataset est de : (30000, 4096, 12)\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_SHAPE = 30000 \n",
    "PATH_TO_GIANT = '/media/cotxetjordi/TOSHIBA EXT/Data_2/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + \"preprocessed_traces15pc.hdf5\", \"r\") as f:\n",
    "    i = 0\n",
    "    signals = []\n",
    "    id_exams = []\n",
    "    ids = list(f[\"id_exam\"][:60000])  #Only 40000 raw data needed\n",
    "\n",
    "    for index,data in tqdm(enumerate(f[\"signal\"])):\n",
    "\n",
    "        id = ids[index]\n",
    "\n",
    "        if id < 2200000: # Only takes exams with annotations available in annotations.csv\n",
    "            \n",
    "            signals.append(data)\n",
    "            id_exams.append(id)\n",
    "            i+=1\n",
    "            \n",
    "        if i == TEST_SHAPE:\n",
    "            break\n",
    "            \n",
    "        \n",
    "signals = np.array(signals)\n",
    "id_exam = np.array(id_exams)\n",
    "print(\"La taille du dataset est de :\", np.shape(signals))\n",
    "print(len(id_exams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e9832-9922-4336-b51b-3bf9ac8beed7",
   "metadata": {},
   "source": [
    "### I.2.a) Save the datas to use it later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed7a879-ba75-46c0-a24f-a10d98f94da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'a') as new_file:\n",
    "    id_exam_data = new_file.create_dataset(name = \"id_exam\",data = id_exams)\n",
    "    signals_data = new_file.create_dataset(name = \"signal\",data= = signals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1de8e3-3b86-424e-86ff-05a308af8bcb",
   "metadata": {},
   "source": [
    "### I.2.b) Load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f746a964-5d55-47fe-bae9-187a30300011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "PATH_TO_GIANT = 'F:/Data_2/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'r') as new_file:\n",
    "    id_exams = list(new_file[\"id_exam\"][()])\n",
    "    X = new_file[\"signal\"][()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f23d20-e7b5-4ed4-88e0-007daefee600",
   "metadata": {},
   "source": [
    "## I.3. Create 7 datasets, one for each disease\n",
    "### 3.1. Find the annotation of each signal of our dataset and divide it into 7 datasets \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62dd2de8-1d86-40b2-a081-c5b3efe45d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['267', '0', '0', '0', '0', '0', '1']\n",
      "19970\n",
      "267\n",
      "{'np': [19970, 16052, 17408, 1836, 12362, 8286], '1dAVB': [], 'RBBB': [], 'LBBB': [], 'SB': [], 'AF': [], 'ST': [19970]}\n",
      "['397', '0', '0', '0', '0', '1', '0']\n",
      "28673\n",
      "397\n",
      "{'np': [19970, 16052, 17408, 1836, 12362, 8286, 2990, 24080], '1dAVB': [], 'RBBB': [], 'LBBB': [], 'SB': [], 'AF': [28673], 'ST': [19970]}\n",
      "['408', '1', '0', '0', '0', '0', '0']\n",
      "5774\n",
      "408\n",
      "{'np': [19970, 16052, 17408, 1836, 12362, 8286, 2990, 24080], '1dAVB': [5774], 'RBBB': [], 'LBBB': [], 'SB': [], 'AF': [28673], 'ST': [19970]}\n",
      "['545', '1', '0', '0', '0', '0', '0']\n",
      "10917\n",
      "545\n",
      "{'np': [19970, 16052, 17408, 1836, 12362, 8286, 2990, 24080, 9350, 23956, 28106], '1dAVB': [5774, 10917], 'RBBB': [], 'LBBB': [], 'SB': [], 'AF': [28673], 'ST': [19970]}\n",
      "['756', '0', '0', '0', '0', '0', '1']\n",
      "8943\n",
      "756\n",
      "{'np': [19970, 16052, 17408, 1836, 12362, 8286, 2990, 24080, 9350, 23956, 28106, 1849, 11953, 11400, 19010], '1dAVB': [5774, 10917], 'RBBB': [], 'LBBB': [], 'SB': [], 'AF': [28673], 'ST': [19970, 8943]}\n",
      "['852', '0', '0', '0', '0', '0', '1']\n",
      "24007\n",
      "852\n",
      "{'np': [19970, 16052, 17408, 1836, 12362, 8286, 2990, 24080, 9350, 23956, 28106, 1849, 11953, 11400, 19010, 12210], '1dAVB': [5774, 10917], 'RBBB': [], 'LBBB': [], 'SB': [], 'AF': [28673], 'ST': [19970, 8943, 24007]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-bec3ff602ed0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                     \u001b[0mCOUNTER\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCOUNTER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import csv\n",
    "PATH_TO = 'F:/Data_2/' #Path to your csv file with only the columns\n",
    "counter = -1\n",
    "start_time = time.time()\n",
    "with open(PATH_TO + 'annotations_processed.csv','r', newline='') as csvfile:\n",
    "        COUNTER = 0\n",
    "        i = 1\n",
    "        carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "        f = csv.reader(csvfile)\n",
    "        indexes = list(carac.keys())\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "            if i != 1:\n",
    "                id_ex = int(line[0])\n",
    "\n",
    "                \n",
    "            if id_ex in id_exams:\n",
    "\n",
    "                if \"1\" in line and line.count(\"1\") == 1 : #Just taking the \"mono-disease\" ECG to analyse\n",
    "                    disease_ids = carac[indexes[line.index(\"1\")]]\n",
    "\n",
    "                    if len(disease_ids) < 2000:\n",
    "                        disease_ids.append(id_exams.index(id_ex))\n",
    "\n",
    "                        COUNTER +=1\n",
    "\n",
    "          \n",
    "               \n",
    "                elif len(carac[\"np\"]) < 2000:\n",
    "                    carac[\"np\"].append(id_exams.index(id_ex))\n",
    "                    COUNTER +=1\n",
    "       \n",
    "            if i % 50000 == 0:\n",
    "                    print(COUNTER)\n",
    "                    print(\"--- %s seconds ---\" % (time.time() - start_time))            \n",
    "            if COUNTER == 12000:\n",
    "                break\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "           \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547425f-c236-4e20-9d19-cef3b0ee7176",
   "metadata": {},
   "source": [
    "### 3.1.bis Create the preprocessed csv annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d2fbc4-cf34-4ee4-811d-6ffd111149ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('F:/Data_2/annotations.csv','r', newline='') as csvfile:\n",
    "    with open('F:/Data_2/annotations_processed.csv','w',newline='') as fichiercsv:\n",
    "        writer=csv.writer(fichiercsv)\n",
    "        i = 0\n",
    "        read = csv.reader(csvfile)\n",
    "        for line in read:\n",
    "\n",
    "\n",
    "                i+=1\n",
    "                writer.writerow([line[0]]+line[4:10]) # Only useful datas for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0344133-1c86-440e-80a3-b45b86a27c78",
   "metadata": {},
   "source": [
    "### I.3.a) Save the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00310adb-2743-4623-9273-797bc2e4adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for index, (dis,ids) in enumerate(carac.items()) :\n",
    "\n",
    "    np.save(dis,ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50c1ae-c4fa-4a0d-a7f3-93268cd64a0b",
   "metadata": {},
   "source": [
    "### I.3.b) Load the 7 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18ec24c7-44bf-4a23-9b7e-4af7b3e2fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np has 941 samples\n",
      "1dAVB has 13 samples\n",
      "RBBB has 37 samples\n",
      "LBBB has 15 samples\n",
      "SB has 13 samples\n",
      "AF has 20 samples\n",
      "ST has 18 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "for index,(key,ids) in enumerate(carac.items()):\n",
    "    \n",
    "    carac[key] = np.load(key+ \".npy\")\n",
    "\n",
    "    print(key,\"has\", len(carac[key]),\"samples\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e353d-d821-4cf4-939c-8bdedf0a3984",
   "metadata": {},
   "source": [
    "### I.4 Create an array composed by 7 arrays of disease-acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0d0bda-a64f-4267-af18-7c831b8c6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all = np.array([[X[index] for index in ids] for ids in carac.values()]) #don't forget to convert each of the 7 lists when you will predict using the pre-trained model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968aa5a5-9815-4144-bdcd-1710f8ece052",
   "metadata": {},
   "source": [
    "## II. Prediction test\n",
    "### II.1 Load and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e65c7d2-8d69-4eb3-b6d1-356f73241537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Import packages\n",
    "import tensorflow.compat.v1 as tf  #Fait appel à la binliothèque Tensorflow Version 1.xx\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "\n",
    "PATH_TO_MODEL  = 'C:/Users/valer/Desktop/Stage_Notebook/automatic-ecg-diagnosis-master/model/'\n",
    "\n",
    "\n",
    "\n",
    "# Import model\n",
    "model = keras.models.load_model(PATH_TO_MODEL+ 'model.hdf5',compile=False)\n",
    "\n",
    "#model = load_model(PATH_TO_MODEL, compile=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(),metrics = [tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc950eb-5b63-4deb-90d6-67a207bb6041",
   "metadata": {},
   "source": [
    "### II.2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "40da69dc-86a3-4e97-a404-e40c07e8ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754189\n",
      "[[2.47979460e+03 1.70598520e+00 1.06970521e+00 2.39693951e+00\n",
      "  3.32541585e+00 1.02417362e+00]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-4f70a1435cd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0msof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msof\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "import math\n",
    "from scipy.special import softmax\n",
    "\n",
    "prediction = softmax(model.predict(np.array([X[12]]))*10*8)\n",
    "print(id_exams[12])\n",
    "u = model.predict(np.array([X[12]]))*10**8\n",
    "sum = 0\n",
    "for i in u :\n",
    "    sum += np.exp(i)\n",
    "\n",
    "sof = np.exp(u)\n",
    "print(sof)\n",
    "print(sum([i for i in sof]))\n",
    "\n",
    "print(softmax(u))\n",
    "\"\"\"for X in X_test_all:\n",
    "    prediction = model.predict(np.array([X[0]]),verbose = 1)\"\"\"\n",
    "print(prediction)\n",
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'r') as new_file:\n",
    "    for i in range(200):\n",
    "        if not (y_final[new_file[\"id_exam\"][i]] != np.array([0,0,0,0,0,0])).all():\n",
    "            print(new_file[\"id_exam\"][i])\n",
    "            print(y_final[new_file[\"id_exam\"][i]] )\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b92d20b1-da47-442a-a7bf-a1d78ae15af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "941/941 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 5ms/step\n",
      "37/37 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step\n",
      "13/13 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:03,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step\n",
      "18/18 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:03,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "Y_test_all = [[[0]*6 for i in range(len(carac[\"np\"] ) ) ] , [[1,0,0,0,0,0]for i in range(len(carac[\"1dAVB\"] ) ) ] , [[0,1,0,0,0,0]for i in range(len(carac[\"RBBB\"] ) ) ] , [[0,0,1,0,0,0]for i in range(len(carac[\"LBBB\"] ) ) ], [[0,0,0,1,0,0]for i in range(len(carac[\"SB\"] ) ) ] , [[1,0,0,0,1,0]for i in range(len(carac[\"AF\"] ) ) ] , [[1,0,0,0,0,1]for i in range(len(carac[\"ST\"] ) ) ] ]\n",
    "for ind,X in tqdm(enumerate(X_test_all)): \n",
    "    results.append(model.evaluate(np.array(X), np.array(Y_test_all[ind]), batch_size=128,verbose = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b23ffa-4276-4847-8968-3f6391e22fe7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-18885004cbea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test_all' is not defined"
     ]
    }
   ],
   "source": [
    "y_score = model.predict(np.array(X_test_all[0]),verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ba0ce4f-3293-4d0c-a32f-2a710b362f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03973692699902623, 0.41001826524734497], [1.870348334312439, 0.409746378660202], [2.414590835571289, 0.4033203125], [2.774968147277832, 0.40038779377937317], [2.66457462310791, 0.39788055419921875], [4.382474422454834, 0.39933207631111145], [5.364100933074951, 0.40066224336624146]]\n"
     ]
    }
   ],
   "source": [
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "697da7a6-758e-4e94-85f8-c8ff84aaffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "[0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "PATH_TO= \"F:/Data_2/\"\n",
    "\n",
    "y_final = []\n",
    "\n",
    "with open(PATH_TO + 'annotations_clean.csv','r', newline='') as csvfile:\n",
    "    f = csv.reader(csvfile)\n",
    "    o=0\n",
    "    print(\"ok\")\n",
    "    for i in f:\n",
    "        if o != 0:\n",
    "\n",
    "            y_final.append([int(z) for z in i])\n",
    "            \n",
    "        o+=1\n",
    "y_final = np.array(y_final)\n",
    "print(y_final[15])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d6c23-a8dc-4122-8731-c5a357471dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
