{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of the diagnosis by the RIBEIRO PRE-TRAINED Model \n",
    "  \n",
    "- On this notebook we wiil look further into the stats of the RIBEIRO Neural Network and confirm the results annonced by his team.  \n",
    "- We will work over **a part** of the giant dataset of more thant 2 millions 12-lead ECG.  \n",
    "- To do so we will take 30000 ECG that will construct our validation dataset. It represents aproximately 1.3% of the whole training dataset\n",
    "- We will create 7 datasets, one for each cardiac disease and the last one if the patient is healthy.  \n",
    "- Also, in order to understand why the prediction is not correct, a {disease}_prob dataset will be created and some of the ECG that causes problem to the machine will be printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Determine diseases datasets  \n",
    "  \n",
    "First we analyse the 1.5%_dataset to find what are the index of the abnormal-ECG, we categorise them into 7 lists.  \n",
    "Those lists will be limited to two thousands indexes, it means that our stats won't be perfect but they will approximate the real ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### I.1. Open datas and take only 30000 samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the annotated ECG in order to compare the prediction to the annotation at the end.  \n",
    "We want **4 or 5 hundreds ECG of each diseases**, then 30000 datas looks alright to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1354it [00:01, 682.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is: (1000, 4096, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_SHAPE = 30000\n",
    "PATH_TO_GIANT = 'path/to/dataset/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + \"preprocessed_traces.hdf5\", \"r\") as f:\n",
    "    i = 0\n",
    "    signals = []\n",
    "    id_exams = []\n",
    "    ids = list(f[\"id_exam\"][:60000])  #Only 60000 raw data needed\n",
    "\n",
    "    for index,data in tqdm(enumerate(f[\"signal\"])):\n",
    "\n",
    "        id = ids[index]\n",
    "\n",
    "        if id < 2200000: # Only takes exams with annotations available in annotations.csv\n",
    "            \n",
    "            signals.append(data)\n",
    "            id_exams.append(id)\n",
    "            i+=1\n",
    "            \n",
    "        if i == TEST_SHAPE:\n",
    "            break\n",
    "            \n",
    "        \n",
    "signals = np.array(signals)\n",
    "id_exam = np.array(id_exams)\n",
    "print(\"Dataset shape is:\", np.shape(signals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.a) Save the datas to use it later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'a') as new_file:\n",
    "    id_exam_data = new_file.create_dataset(name = \"id_exam\",data = id_exams)\n",
    "    signals_data = new_file.create_dataset(name = \"signal\",data = signals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.b) Load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "PATH_TO_GIANT = 'path/to/dataset'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'r') as new_file:\n",
    "    id_exams = list(new_file[\"id_exam\"][()])\n",
    "    X = new_file[\"signal\"][()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3. Create 7 datasets, one for each disease\n",
    "\n",
    "### I.3.a) Find the annotation of each signal of our dataset and divide it into 7 datasets\n",
    "<span style=\"color:blue\">Note : If you downloaded the disesases folder on GitHub (https://github.com/jordicotxet/ECG-diagnosis-by-Neural-Network/tree/main/diseases), you can skip this part and go directly to **I.2.c** </span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "--- 13.818173885345459 seconds ---\n",
      "85\n",
      "--- 26.667856216430664 seconds ---\n",
      "113\n",
      "--- 40.73766040802002 seconds ---\n",
      "150\n",
      "--- 55.54013228416443 seconds ---\n",
      "179\n",
      "--- 67.33786916732788 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2d8dcb389246>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                         \u001b[0mCOUNTER\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCOUNTER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "PATH_TO = 'path/to/csv' #Path to your csv file with only the columns\n",
    "counter = -1\n",
    "start_time = time.time()\n",
    "with open(PATH_TO + 'annotations_processed.csv','r', newline='') as csvfile:\n",
    "        COUNTER = 0\n",
    "        i = 1\n",
    "        carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "        f = csv.reader(csvfile)\n",
    "        indexes = list(carac.keys())\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "            if i != 1:\n",
    "                id_ex = int(line[0])\n",
    "\n",
    "                \n",
    "                if id_ex in id_exams:\n",
    "\n",
    "                    if \"1\" in line and line.count(\"1\") == 1 : #Just taking the \"mono-disease\" ECG to analyse\n",
    "                        disease_ids = carac[indexes[line.index(\"1\")]]\n",
    "\n",
    "                        if len(disease_ids) < 2000:\n",
    "                            disease_ids.append(id_exams.index(id_ex))\n",
    "\n",
    "                            COUNTER +=1\n",
    "\n",
    "\n",
    "\n",
    "                    elif len(carac[\"np\"]) < 2000:\n",
    "                        carac[\"np\"].append(id_exams.index(id_ex))\n",
    "                        COUNTER +=1\n",
    "       \n",
    "            if i % 1000000 == 0: #Just to have a look of the progression\n",
    "                    print(COUNTER)\n",
    "                    print(\"--- %s seconds ---\" % (time.time() - start_time))     \n",
    "                    \n",
    "            if COUNTER == 5500:\n",
    "                break\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "           \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3.a)bis Create the preprocessed csv annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('/path/to/csv'+'annotations.csv','r', newline='') as csvfile:\n",
    "    with open(\"/path/to/csv\"+'annotations_processed.csv','w',newline='') as fichiercsv:\n",
    "        writer=csv.writer(fichiercsv)\n",
    "        i = 0\n",
    "        read = csv.reader(csvfile)\n",
    "        for line in read:\n",
    "                i+=1\n",
    "                writer.writerow([line[0]]+line[4:10]) # Only useful datas for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3.b) Save the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for index, (dis,ids) in enumerate(carac.items()) :\n",
    "\n",
    "    np.save(dis,ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3.c) Load the 7 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np has 2000 samples\n",
      "1dAVB has 365 samples\n",
      "RBBB has 605 samples\n",
      "LBBB has 392 samples\n",
      "SB has 416 samples\n",
      "AF has 466 samples\n",
      "ST has 610 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path_to_dis = \"/path/to/disease\"\n",
    "\n",
    "carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "for index,(key,ids) in enumerate(carac.items()):\n",
    "    \n",
    "    carac[key] = np.load(path_to_dis + key + \".npy\")\n",
    "\n",
    "    print(key,\"has\", len(carac[key]),\"samples\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4 Create an array composed by 7 arrays of disease-acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all = np.array([[X[index] for index in ids] for ids in carac.values()]) #don't forget to convert each of the 7 lists when you will predict using the pre-trained model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Prediction test\n",
    "### II.1 Load and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Import packages\n",
    "import tensorflow.compat.v1 as tf  #Fait appel à la binliothèque Tensorflow Version 1.xx\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "\n",
    "PATH_TO_MODEL  = 'Path/to/ribeiro/model'\n",
    "\n",
    "\n",
    "\n",
    "# Import model\n",
    "model = keras.models.load_model(PATH_TO_MODEL+ 'model.hdf5',compile=False)\n",
    "\n",
    "#model = load_model(PATH_TO_MODEL, compile=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(),metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Prediction test\n",
    "### II.2.a) Auto-evaluation\n",
    "If we use the Keras-pre-implemented function **\"evaluate\"** we can see that the prediction score is not as good as announced. That is why we will **manually** determine the tresholds and practice our tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 7s 4ms/step\n",
      "365/365 [==============================] - 1s 3ms/step\n",
      "605/605 [==============================] - 2s 3ms/step\n",
      "392/392 [==============================] - 1s 4ms/step\n",
      "416/416 [==============================] - 1s 4ms/step\n",
      "466/466 [==============================] - 2s 3ms/step\n",
      "610/610 [==============================] - 2s 3ms/step\n",
      "[0.5453733801841736, 0.5464203953742981, 0.5529597997665405, 0.5596546530723572, 0.5647485852241516, 0.5611258149147034, 0.5523701906204224]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "y_true = [[[0]*6 for i in range(len(carac[\"np\"] ) ) ] , [[1,0,0,0,0,0]for i in range(len(carac[\"1dAVB\"] ) ) ] , [[0,1,0,0,0,0]for i in range(len(carac[\"RBBB\"] ) ) ] , [[0,0,1,0,0,0]for i in range(len(carac[\"LBBB\"] ) ) ], [[0,0,0,1,0,0]for i in range(len(carac[\"SB\"] ) ) ] , [[1,0,0,0,1,0]for i in range(len(carac[\"AF\"] ) ) ] , [[1,0,0,0,0,1]for i in range(len(carac[\"ST\"] ) ) ] ]\n",
    "for ind,X in enumerate(X_test_all): \n",
    "    results.append(model.evaluate(np.array(X), np.array(y_true[ind]), batch_size=128,verbose = 1))\n",
    "print([i[1] for i in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.b)i. Prediction over our mono-disease array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 7s 3ms/step\n",
      "365/365 [==============================] - 1s 4ms/step\n",
      "605/605 [==============================] - 2s 3ms/step\n",
      "392/392 [==============================] - 1s 3ms/step\n",
      "416/416 [==============================] - 1s 3ms/step\n",
      "466/466 [==============================] - 2s 3ms/step\n",
      "610/610 [==============================] - 2s 3ms/step\n",
      "Prediction saved!\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "y_score = []\n",
    "lens = [len(carac[dise]) for dise in carac.keys()] #the number of samples for each disease\n",
    "y_true = [[[0]*6 for i in range(lens[0] )  ] + [[1,0,0,0,0,0]for i in range(lens[1] ) ] + [[0,1,0,0,0,0]for i in range(lens[2] )  ] + [[0,0,1,0,0,0]for i in range(lens[3] ) ]+ [[0,0,0,1,0,0]for i in range(lens[4] ) ] + [[0,0,0,0,1,0]for i in range(lens[5] ) ] + [[0,0,0,0,0,1]for i in range(lens[6]) ] ]\n",
    "for X in X_test_all:    \n",
    "    y_score += list(model.predict(np.array(X),verbose =1))\n",
    "print(\"Prediction saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.b)ii. Get optimal precision and recall using the best thresholds\n",
    "<span style=\"color:red\"> Here we have a problem with the **first disease** we can see that the machine precision is really bad compared to others.  \n",
    "So, let's determine the precision by ourselves. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.67234043 0.92834395 0.86774942 0.8800905  0.92393736 0.92868217]\n",
      "recall: [0.86575342 0.96363636 0.95408163 0.93509615 0.88626609 0.98196721]\n",
      "thresholds: [0.17756437 0.22543368 0.2129037  0.19892462 0.32807842 0.15627445]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (confusion_matrix,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "y_true = np.array(y_true[0])\n",
    "y_score = np.array(y_score)\n",
    "def get_optimal_precision_recall(y_true, y_score):\n",
    "    \"\"\"Find precision and recall values that maximize f1 score.\"\"\"\n",
    "    n = np.shape(y_true)[1]\n",
    "    opt_precision = []\n",
    "    opt_recall = []\n",
    "    opt_threshold = []\n",
    "    for k in range(n):\n",
    "        # Get precision-recall curve\n",
    "        precision, recall, threshold = precision_recall_curve(y_true[:, k], y_score[:, k])\n",
    "        # Compute f1 score for each point (use nan_to_num to avoid nans messing up the results)\n",
    "        f1_score = np.nan_to_num(2 * precision * recall / (precision + recall))\n",
    "        # Select threshold that maximize f1 score\n",
    "        index = np.argmax(f1_score)\n",
    "        opt_precision.append(precision[index])\n",
    "        opt_recall.append(recall[index])\n",
    "        t = threshold[index-1] if index != 0 else threshold[0]-1e-10\n",
    "        opt_threshold.append(t)\n",
    "\n",
    "    return np.array(opt_precision), np.array(opt_recall), np.array(opt_threshold)\n",
    "\n",
    "p,r,t = get_optimal_precision_recall(y_true,y_score)\n",
    "print(\"precision:\",p)\n",
    "print(\"recall:\",r)\n",
    "print('thresholds:',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.c) Convert to one hot encoding\n",
    "We chose our threshold adaptated to this dataset, then we convert our **raw prediction into one hot encoding predictions** in order to compare it to the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4854/4854 [00:00<00:00, 146956.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "y_score_conv = []\n",
    "thresholds = t\n",
    "\n",
    "\n",
    "for pred in tqdm.tqdm(y_score):\n",
    "    res = [0,0,0,0,0,0]\n",
    "    for ind,val in enumerate(pred):\n",
    "\n",
    "        if val >= thresholds[ind]:\n",
    "           \n",
    "            res[ind] = 1 \n",
    "\n",
    "    y_score_conv.append(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. Determine how much predictions errors are made and what are the ECGs that cause those errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "lens_ok = []\n",
    "for i,len_ in enumerate(lens):\n",
    "    if i == 0:\n",
    "        lens_ok.append(len_)\n",
    "    else: \n",
    "        lens_ok.append(lens_ok[i-1] + lens[i])\n",
    "\n",
    "pos = 0\n",
    "failed_prec = []\n",
    "sensi =  0\n",
    "failed_sensi = []\n",
    "nb_fail = 0\n",
    "\n",
    "for last_samp in lens_ok:\n",
    "        failed_dis = []\n",
    "        failed_sensi_dis = []\n",
    "        for i in range(pos,last_samp):\n",
    "            if not (y_true[i] == y_score_conv[i]).all(): \n",
    "                failed_dis.append(i)\n",
    "                nb_fail += 1\n",
    "\n",
    "                if i > lens_ok[0]: #Only look at the prediction with disease\n",
    "                    try:\n",
    "                        index_dis = list(y_true[i]).index(1)\n",
    "                        if y_score_conv[i][index_dis] != 1:\n",
    "                            sensi += 1\n",
    "                            failed_sensi_dis.append(i) \n",
    "                            \n",
    "                    except ValueError:#If no disease is predicted\n",
    "                        sensi += 1\n",
    "                        failed_sensi_dis.append(i) \n",
    "                    \n",
    "        failed_sensi.append(failed_sensi_dis)        \n",
    "        failed_prec.append(failed_dis)\n",
    "        pos = i \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4. Create precision datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------/TOTAL\\---------------------------------\n",
      "Over a number of 4900 samples the prediction failed: 495 times\n",
      "It means the total precision is 89.8 %\n",
      "--/AND\\--\n",
      "Over a number of 4900 samples the prediction failed to detect a disease: 181 times\n",
      "It means the total sensitivity is 96.3 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/np\\-------------------------------\n",
      "Over a number of 2000 samples of np ECG the prediction failed to detect a disease: 161 times\n",
      "It means the precision is 92 %\n",
      "Examples of errors: [(8, [0, 0, 0, 1, 0, 0]), (9, [1, 0, 0, 0, 0, 0]), (13, [1, 0, 1, 0, 0, 0]), (19, [0, 0, 0, 1, 0, 0]), (22, [0, 0, 0, 0, 1, 0]), (43, [1, 0, 0, 0, 0, 0]), (47, [0, 0, 1, 0, 0, 1]), (48, [1, 0, 0, 1, 0, 0]), (72, [1, 0, 0, 0, 0, 0]), (86, [0, 0, 0, 0, 0, 1])] instead of [0 0 0 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 2000 samples of np ECG the prediction failed to detect a disease: 0 times\n",
      "It means the sensitivity is 100 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/1dAVB\\-------------------------------\n",
      "Over a number of 365 samples of 1dAVB ECG the prediction failed to detect a disease: 65 times\n",
      "It means the precision is 82.2 %\n",
      "Examples of errors: [(2011, [0, 0, 0, 0, 0, 0]), (2013, [0, 0, 0, 0, 0, 0]), (2014, [1, 0, 0, 1, 0, 0]), (2016, [0, 0, 0, 0, 0, 0]), (2021, [0, 0, 0, 0, 0, 0]), (2030, [0, 0, 0, 0, 0, 0]), (2039, [1, 0, 1, 0, 0, 0]), (2044, [0, 0, 0, 0, 0, 0]), (2048, [1, 0, 1, 0, 0, 0]), (2059, [0, 0, 0, 0, 1, 0])] instead of [1 0 0 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 365 samples of 1dAVB ECG the prediction failed to detect a disease: 49 times\n",
      "It means the sensitivity is 86.6 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/RBBB\\-------------------------------\n",
      "Over a number of 605 samples of RBBB ECG the prediction failed to detect a disease: 77 times\n",
      "It means the precision is 87.3 %\n",
      "Examples of errors: [(2367, [0, 0, 0, 1, 0, 0]), (2370, [0, 1, 0, 0, 1, 1]), (2372, [1, 1, 0, 1, 0, 0]), (2374, [0, 0, 0, 0, 0, 0]), (2380, [0, 1, 0, 0, 0, 1]), (2404, [1, 1, 0, 1, 0, 0]), (2413, [0, 0, 0, 0, 0, 0]), (2420, [1, 1, 0, 0, 0, 0]), (2442, [1, 1, 0, 0, 0, 0]), (2453, [1, 1, 0, 0, 0, 0])] instead of [0 1 0 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 605 samples of RBBB ECG the prediction failed to detect a disease: 22 times\n",
      "It means the sensitivity is 96.4 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/LBBB\\-------------------------------\n",
      "Over a number of 392 samples of LBBB ECG the prediction failed to detect a disease: 54 times\n",
      "It means the precision is 86.2 %\n",
      "Examples of errors: [(2969, [0, 0, 1, 0, 0, 0]), (2981, [0, 0, 0, 0, 0, 0]), (2987, [1, 0, 1, 0, 0, 0]), (2990, [0, 0, 1, 1, 0, 0]), (2995, [0, 0, 0, 0, 1, 0]), (3001, [1, 0, 1, 0, 0, 0]), (3019, [1, 0, 1, 0, 0, 0]), (3023, [0, 0, 0, 0, 0, 0]), (3045, [1, 0, 1, 0, 0, 0]), (3052, [1, 0, 1, 0, 0, 0])] instead of [0 0 1 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 392 samples of LBBB ECG the prediction failed to detect a disease: 19 times\n",
      "It means the sensitivity is 95.2 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/SB\\-------------------------------\n",
      "Over a number of 416 samples of SB ECG the prediction failed to detect a disease: 41 times\n",
      "It means the precision is 90.1 %\n",
      "Examples of errors: [(3372, [0, 1, 0, 1, 0, 0]), (3391, [1, 0, 0, 1, 0, 0]), (3393, [0, 0, 0, 0, 0, 0]), (3411, [0, 0, 0, 0, 0, 0]), (3437, [0, 0, 0, 0, 0, 0]), (3440, [0, 0, 0, 0, 0, 0]), (3444, [1, 0, 0, 1, 0, 0]), (3454, [0, 0, 0, 0, 0, 0]), (3463, [0, 0, 0, 0, 0, 0]), (3469, [0, 0, 0, 0, 0, 0])] instead of [0 0 0 1 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 416 samples of SB ECG the prediction failed to detect a disease: 27 times\n",
      "It means the sensitivity is 93.5 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/AF\\-------------------------------\n",
      "Over a number of 466 samples of AF ECG the prediction failed to detect a disease: 71 times\n",
      "It means the precision is 84.8 %\n",
      "Examples of errors: [(3794, [0, 0, 0, 0, 0, 0]), (3812, [0, 0, 1, 0, 1, 0]), (3818, [0, 0, 0, 0, 0, 0]), (3838, [0, 0, 0, 0, 0, 0]), (3843, [0, 0, 1, 0, 1, 0]), (3849, [0, 0, 0, 0, 1, 1]), (3857, [1, 1, 0, 0, 0, 0]), (3859, [0, 0, 0, 0, 0, 1]), (3860, [0, 0, 0, 1, 0, 0]), (3864, [0, 0, 0, 0, 0, 0])] instead of [0 0 0 0 1 0]\n",
      "--/AND\\--\n",
      "Over a number of 466 samples of AF ECG the prediction failed to detect a disease: 53 times\n",
      "It means the sensitivity is 88.6 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/ST\\-------------------------------\n",
      "Over a number of 610 samples of ST ECG the prediction failed to detect a disease: 26 times\n",
      "It means the precision is 95.7 %\n",
      "Examples of errors: [(4261, [0, 0, 0, 0, 1, 1]), (4293, [0, 0, 0, 0, 1, 1]), (4294, [0, 0, 0, 0, 0, 0]), (4309, [0, 0, 0, 0, 0, 0]), (4315, [1, 0, 0, 0, 0, 1]), (4341, [1, 0, 0, 0, 0, 1]), (4363, [0, 1, 0, 0, 0, 1]), (4372, [0, 1, 0, 0, 0, 1]), (4409, [1, 0, 0, 0, 0, 1]), (4477, [0, 0, 0, 0, 0, 0])] instead of [0 0 0 0 0 1]\n",
      "--/AND\\--\n",
      "Over a number of 610 samples of ST ECG the prediction failed to detect a disease: 11 times\n",
      "It means the sensitivity is 98.2 %\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------/TOTAL\\---------------------------------\")\n",
    "print(\"Over a number of 4900 samples the prediction failed:\",nb_fail,\"times\")\n",
    "print(\"It means the total precision is {0:.3g} %\".format((1-(nb_fail/(pos+1)))*100))\n",
    "print(\"--/AND\\--\")\n",
    "print(\"Over a number of 4900 samples the prediction failed to detect a disease:\",sensi,\"times\")\n",
    "print(\"It means the total sensitivity is {0:.3g} %\".format((1-(sensi/(pos+1)))*100))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "for ind,fails in enumerate(failed_prec):\n",
    "    print(\"-------------------------------/{}\\-------------------------------\".format(list(carac.keys())[ind]))\n",
    "    print(\"Over a number of {} samples of {} ECG the prediction failed to detect a disease: {} times\".format(lens[ind],list(carac.keys())[ind],len(fails)))\n",
    "    print(\"It means the precision is {0:.3g} %\".format((1-(len(fails)/lens[ind]))*100))\n",
    "    print('Examples of errors: {} instead of {}'.format([(i,y_score_conv[i]) for i in fails[:10]],y_true[lens_ok[ind]-1]))\n",
    "    print(\"--/AND\\--\")\n",
    "    print(\"Over a number of {} samples of {} ECG the prediction failed to detect a disease: {} times\".format(lens[ind],list(carac.keys())[ind],len(failed_sensi[ind])))\n",
    "    print(\"It means the sensitivity is {0:.3g} %\".format((1-(len(failed_sensi[ind])/lens[ind]))*100))\n",
    "    print(\"------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
