{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf664b3-147f-4796-93bc-f6a8a4724442",
   "metadata": {},
   "source": [
    "# Verification of the diagnosis by the RIBEIRO PRE-TRAINED Model \n",
    "  \n",
    "- On this notebook we wiil look further into the stats of the RIBEIRO Neural Network and confirm the results annonced by his team.  \n",
    "- We will work over **a part** of the giant dataset of more thant 2 millions 12-lead ECG.  \n",
    "- To do so we will take 30000 ECG that will construct our validation dataset. It represents aproximately 1.3% of the whole training dataset\n",
    "- We will create 7 datasets, one for each cardiac disease and the last one if the patient is healthy.  \n",
    "- Also, in order to understand why the prediction is not correct, a {disease}_prob dataset will be created and some of the ECG that causes problem to the machine will be printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ec975-2923-4289-bae7-cf03acd30e03",
   "metadata": {},
   "source": [
    "# I. Determine diseases datasets  \n",
    "  \n",
    "First we analyse the 1.5%_dataset to find what are the index of the abnormal-ECG, we categorise them into 7 lists.  \n",
    "Those lists will be limited to two thousands indexes, it means that our stats won't be perfect but they will approximate the real ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68730148-0341-43ed-852c-f8b9f596d1b2",
   "metadata": {},
   "source": [
    "  ### I.1. Open datas and take only 30000 samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd656b-4ec1-4752-b434-90647dc0b34e",
   "metadata": {},
   "source": [
    "Taking the annotated ECG in order to compare the prediction to the annotation at the end.  \n",
    "We want **4 or 5 hundreds ECG of each diseases**, then 30000 datas looks alright to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1752fd7c-6cca-4c12-8850-5ec307336843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40635it [00:07, 5340.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du dataset est de : (30000, 4096, 12)\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_SHAPE = 30000 \n",
    "PATH_TO_GIANT = '/media/cotxetjordi/TOSHIBA EXT/Data_2/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + \"preprocessed_traces15pc.hdf5\", \"r\") as f:\n",
    "    i = 0\n",
    "    signals = []\n",
    "    id_exams = []\n",
    "    ids = list(f[\"id_exam\"][:60000])  #Only 40000 raw data needed\n",
    "\n",
    "    for index,data in tqdm(enumerate(f[\"signal\"])):\n",
    "\n",
    "        id = ids[index]\n",
    "\n",
    "        if id < 2200000: # Only takes exams with annotations available in annotations.csv\n",
    "            \n",
    "            signals.append(data)\n",
    "            id_exams.append(id)\n",
    "            i+=1\n",
    "            \n",
    "        if i == TEST_SHAPE:\n",
    "            break\n",
    "            \n",
    "        \n",
    "signals = np.array(signals)\n",
    "id_exam = np.array(id_exams)\n",
    "print(\"La taille du dataset est de :\", np.shape(signals))\n",
    "print(len(id_exams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e9832-9922-4336-b51b-3bf9ac8beed7",
   "metadata": {},
   "source": [
    "### I.2.a) Save the datas to use it later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed7a879-ba75-46c0-a24f-a10d98f94da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'a') as new_file:\n",
    "    id_exam_data = new_file.create_dataset(name = \"id_exam\",data = id_exams)\n",
    "    signals_data = new_file.create_dataset(name = \"signal\",data= = signals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1de8e3-3b86-424e-86ff-05a308af8bcb",
   "metadata": {},
   "source": [
    "### I.2.b) Load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f746a964-5d55-47fe-bae9-187a30300011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "PATH_TO_GIANT = 'F:/Data_2/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'r') as new_file:\n",
    "    id_exams = list(new_file[\"id_exam\"][()])\n",
    "    X = new_file[\"signal\"][()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f23d20-e7b5-4ed4-88e0-007daefee600",
   "metadata": {},
   "source": [
    "## I.3. Create 7 datasets, one for each disease\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dd2de8-1d86-40b2-a081-c5b3efe45d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_ex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4206eaac6138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mid_ex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mid_exams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"1\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;31m#Just taking the \"mono-disease\" ECG to analyse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'id_ex' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "PATH_TO = 'F:/Data_2/' #Path to your csv file with only the columns\n",
    "counter = -1\n",
    "start_time = time.time()\n",
    "with open(PATH_TO + 'annotations_processed.csv','r', newline='') as csvfile:\n",
    "        COUNTER = 0\n",
    "        i = 1\n",
    "        carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "        f = csv.reader(csvfile)\n",
    "        indexes = list(carac.keys())\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "            if i != 1:\n",
    "                id_ex = int(line[0])\n",
    "\n",
    "                \n",
    "            if id_ex in id_exams:\n",
    "\n",
    "                if \"1\" in line and line.count(\"1\") == 1 : #Just taking the \"mono-disease\" ECG to analyse\n",
    "                    disease_ids = carac[indexes[line.index(\"1\")]]\n",
    "\n",
    "                    if len(disease_ids) < 2000:\n",
    "                        disease_ids.append(id_exams.index(id_ex))\n",
    "\n",
    "                        COUNTER +=1\n",
    "\n",
    "          \n",
    "               \n",
    "                elif len(carac[\"np\"]) < 2000:\n",
    "                    carac[\"np\"].append(id_exams.index(id_ex))\n",
    "                    COUNTER +=1\n",
    "       \n",
    "            if i % 50000 == 0:\n",
    "                    print(COUNTER)\n",
    "                    print(\"--- %s seconds ---\" % (time.time() - start_time))     \n",
    "                    \n",
    "            if COUNTER == 5500:\n",
    "                break\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "           \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547425f-c236-4e20-9d19-cef3b0ee7176",
   "metadata": {},
   "source": [
    "### I.3.a)bis Create the preprocessed csv annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26d2fbc4-cf34-4ee4-811d-6ffd111149ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('F:/Data_2/annotations.csv','r', newline='') as csvfile:\n",
    "    with open('F:/Data_2/annotations_processed.csv','w',newline='') as fichiercsv:\n",
    "        writer=csv.writer(fichiercsv)\n",
    "        i = 0\n",
    "        read = csv.reader(csvfile)\n",
    "        for line in read:\n",
    "\n",
    "\n",
    "                i+=1\n",
    "                writer.writerow([line[0]]+line[4:10]) # Only useful datas for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0344133-1c86-440e-80a3-b45b86a27c78",
   "metadata": {},
   "source": [
    "### I.3.b) Save the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00310adb-2743-4623-9273-797bc2e4adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for index, (dis,ids) in enumerate(carac.items()) :\n",
    "\n",
    "    np.save(dis,ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50c1ae-c4fa-4a0d-a7f3-93268cd64a0b",
   "metadata": {},
   "source": [
    "### I.3.c) Load the 7 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ec24c7-44bf-4a23-9b7e-4af7b3e2fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np has 2000 samples\n",
      "1dAVB has 365 samples\n",
      "RBBB has 605 samples\n",
      "LBBB has 392 samples\n",
      "SB has 416 samples\n",
      "AF has 466 samples\n",
      "ST has 610 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path_to_dis = \"F:/Data_2/diseases/\"\n",
    "\n",
    "carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "for index,(key,ids) in enumerate(carac.items()):\n",
    "    \n",
    "    carac[key] = np.load(path_to_dis + key + \".npy\")\n",
    "\n",
    "    print(key,\"has\", len(carac[key]),\"samples\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e353d-d821-4cf4-939c-8bdedf0a3984",
   "metadata": {},
   "source": [
    "### I.4 Create an array composed by 7 arrays of disease-acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0d0bda-a64f-4267-af18-7c831b8c6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all = np.array([[X[index] for index in ids] for ids in carac.values()]) #don't forget to convert each of the 7 lists when you will predict using the pre-trained model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968aa5a5-9815-4144-bdcd-1710f8ece052",
   "metadata": {},
   "source": [
    "## II. Prediction test\n",
    "### II.1 Load and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e65c7d2-8d69-4eb3-b6d1-356f73241537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tfradeon\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Import packages\n",
    "import tensorflow.compat.v1 as tf  #Fait appel à la binliothèque Tensorflow Version 1.xx\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "\n",
    "PATH_TO_MODEL  = 'C:/Users/valer/Desktop/Stage_Notebook/automatic-ecg-diagnosis-master/model/'\n",
    "\n",
    "\n",
    "\n",
    "# Import model\n",
    "model = keras.models.load_model(PATH_TO_MODEL+ 'model.hdf5',compile=False)\n",
    "\n",
    "#model = load_model(PATH_TO_MODEL, compile=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(),metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc950eb-5b63-4deb-90d6-67a207bb6041",
   "metadata": {},
   "source": [
    "### II.2. Prediction test\n",
    "### II.2.a) Auto-evaluation\n",
    "If we use the Keras-pre-implemented function **\"evaluate\"** we can see that the prediction score is not as good as announced. That is why we will **manually** determine the tresholds and practice our tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b92d20b1-da47-442a-a7bf-a1d78ae15af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 7s 4ms/step\n",
      "365/365 [==============================] - 1s 3ms/step\n",
      "605/605 [==============================] - 2s 3ms/step\n",
      "392/392 [==============================] - 1s 4ms/step\n",
      "416/416 [==============================] - 1s 4ms/step\n",
      "466/466 [==============================] - 2s 3ms/step\n",
      "610/610 [==============================] - 2s 3ms/step\n",
      "[0.5453733801841736, 0.5464203953742981, 0.5529597997665405, 0.5596546530723572, 0.5647485852241516, 0.5611258149147034, 0.5523701906204224]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "y_true = [[[0]*6 for i in range(len(carac[\"np\"] ) ) ] , [[1,0,0,0,0,0]for i in range(len(carac[\"1dAVB\"] ) ) ] , [[0,1,0,0,0,0]for i in range(len(carac[\"RBBB\"] ) ) ] , [[0,0,1,0,0,0]for i in range(len(carac[\"LBBB\"] ) ) ], [[0,0,0,1,0,0]for i in range(len(carac[\"SB\"] ) ) ] , [[1,0,0,0,1,0]for i in range(len(carac[\"AF\"] ) ) ] , [[1,0,0,0,0,1]for i in range(len(carac[\"ST\"] ) ) ] ]\n",
    "for ind,X in enumerate(X_test_all): \n",
    "    results.append(model.evaluate(np.array(X), np.array(y_true[ind]), batch_size=128,verbose = 1))\n",
    "print([i[1] for i in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293a80b-c8a1-4bce-b0ff-c316ec22921e",
   "metadata": {},
   "source": [
    "### II.2.b)i. Prediction over our mono-disease array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b23ffa-4276-4847-8968-3f6391e22fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 7s 3ms/step\n",
      "365/365 [==============================] - 1s 4ms/step\n",
      "605/605 [==============================] - 2s 3ms/step\n",
      "392/392 [==============================] - 1s 3ms/step\n",
      "416/416 [==============================] - 1s 3ms/step\n",
      "466/466 [==============================] - 2s 3ms/step\n",
      "610/610 [==============================] - 2s 3ms/step\n",
      "Prediction saved!\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "y_score = []\n",
    "lens = [len(carac[dise]) for dise in carac.keys()] #the number of samples for each disease\n",
    "y_true = [[[0]*6 for i in range(lens[0] )  ] + [[1,0,0,0,0,0]for i in range(lens[1] ) ] + [[0,1,0,0,0,0]for i in range(lens[2] )  ] + [[0,0,1,0,0,0]for i in range(lens[3] ) ]+ [[0,0,0,1,0,0]for i in range(lens[4] ) ] + [[0,0,0,0,1,0]for i in range(lens[5] ) ] + [[0,0,0,0,0,1]for i in range(lens[6]) ] ]\n",
    "for X in X_test_all:    \n",
    "    y_score += list(model.predict(np.array(X),verbose =1))\n",
    "print(\"Prediction saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbf9d5-4dc8-41a8-b71c-69c2b7c63466",
   "metadata": {},
   "source": [
    "### II.2.b)ii. Get optimal precision and recall using the best thresholds\n",
    "<span style=\"color:red\"> Here we have a problem with the **first disease** we can see that the machine precision is really bad compared to others.  \n",
    "So, let's determine the precision by ourselves. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba0ce4f-3293-4d0c-a32f-2a710b362f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.67234043 0.92834395 0.86774942 0.8800905  0.92393736 0.92868217]\n",
      "recall: [0.86575342 0.96363636 0.95408163 0.93509615 0.88626609 0.98196721]\n",
      "thresholds: [0.17756437 0.22543368 0.2129037  0.19892462 0.32807842 0.15627445]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (confusion_matrix,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "y_true = np.array(y_true[0])\n",
    "y_score = np.array(y_score)\n",
    "def get_optimal_precision_recall(y_true, y_score):\n",
    "    \"\"\"Find precision and recall values that maximize f1 score.\"\"\"\n",
    "    n = np.shape(y_true)[1]\n",
    "    opt_precision = []\n",
    "    opt_recall = []\n",
    "    opt_threshold = []\n",
    "    for k in range(n):\n",
    "        # Get precision-recall curve\n",
    "        precision, recall, threshold = precision_recall_curve(y_true[:, k], y_score[:, k])\n",
    "        # Compute f1 score for each point (use nan_to_num to avoid nans messing up the results)\n",
    "        f1_score = np.nan_to_num(2 * precision * recall / (precision + recall))\n",
    "        # Select threshold that maximize f1 score\n",
    "        index = np.argmax(f1_score)\n",
    "        opt_precision.append(precision[index])\n",
    "        opt_recall.append(recall[index])\n",
    "        t = threshold[index-1] if index != 0 else threshold[0]-1e-10\n",
    "        opt_threshold.append(t)\n",
    "\n",
    "    return np.array(opt_precision), np.array(opt_recall), np.array(opt_threshold)\n",
    "\n",
    "p,r,t = get_optimal_precision_recall(y_true,y_score)\n",
    "print(\"precision:\",p)\n",
    "print(\"recall:\",r)\n",
    "print('thresholds:',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47880e7-eca0-4bf2-acd3-f174f50b6d89",
   "metadata": {},
   "source": [
    "### II.2.c) Convert to one hot encoding\n",
    "We chose our threshold adaptated to this dataset, then we convert our **raw prediction into one hot encoding predictions** in order to compare it to the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9e64581-a611-45ce-9298-c5dd6dc79472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4854/4854 [00:00<00:00, 146956.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "y_score_conv = []\n",
    "thresholds = t\n",
    "\n",
    "\n",
    "for pred in tqdm.tqdm(y_score):\n",
    "    res = [0,0,0,0,0,0]\n",
    "    for ind,val in enumerate(pred):\n",
    "\n",
    "        if val >= thresholds[ind]:\n",
    "           \n",
    "            res[ind] = 1 \n",
    "\n",
    "    y_score_conv.append(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543f2be-7c2c-4588-af6d-8b517c8814f4",
   "metadata": {},
   "source": [
    "### II.3. Determine how much predictions errors are made and what are the ECGs that cause those errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ff0a467-d208-455d-b00e-bdbd81d1f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "lens_ok = []\n",
    "for i,len_ in enumerate(lens):\n",
    "    if i == 0:\n",
    "        lens_ok.append(len_)\n",
    "    else: \n",
    "        lens_ok.append(lens_ok[i-1] + lens[i])\n",
    "\n",
    "pos = 0\n",
    "failed_prec = []\n",
    "sensi =  0\n",
    "failed_sensi = []\n",
    "nb_fail = 0\n",
    "\n",
    "for last_samp in lens_ok:\n",
    "        failed_dis = []\n",
    "        failed_sensi_dis = []\n",
    "        for i in range(pos,last_samp):\n",
    "            if not (y_true[i] == y_score_conv[i]).all(): \n",
    "                failed_dis.append(i)\n",
    "                nb_fail += 1\n",
    "\n",
    "                if i > lens_ok[0]: #Only look at the prediction with disease\n",
    "                    try:\n",
    "                        index_dis = list(y_true[i]).index(1)\n",
    "                        if y_score_conv[i][index_dis] != 1:\n",
    "                            sensi += 1\n",
    "                            failed_sensi_dis.append(i) \n",
    "                            \n",
    "                    except ValueError:#If no disease is predicted\n",
    "                        sensi += 1\n",
    "                        failed_sensi_dis.append(i) \n",
    "                    \n",
    "        failed_sensi.append(failed_sensi_dis)        \n",
    "        failed_prec.append(failed_dis)\n",
    "        pos = i \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ad0dd-43ac-4da3-8dfe-f200f6845b33",
   "metadata": {},
   "source": [
    "### II.4. Create precision datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbb6ae96-8d48-4cbf-9179-6b981d1804f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------/TOTAL\\---------------------------------\n",
      "Over a number of 4900 samples the prediction failed: 495 times\n",
      "It means the total precision is 89.8 %\n",
      "--/AND\\--\n",
      "Over a number of 4900 samples the prediction failed to detect a disease: 181 times\n",
      "It means the total sensitivity is 96.3 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/np\\-------------------------------\n",
      "Over a number of 2000 samples of np ECG the prediction failed to detect a disease: 161 times\n",
      "It means the precision is 92 %\n",
      "Examples of errors: [(8, [0, 0, 0, 1, 0, 0]), (9, [1, 0, 0, 0, 0, 0]), (13, [1, 0, 1, 0, 0, 0]), (19, [0, 0, 0, 1, 0, 0]), (22, [0, 0, 0, 0, 1, 0]), (43, [1, 0, 0, 0, 0, 0]), (47, [0, 0, 1, 0, 0, 1]), (48, [1, 0, 0, 1, 0, 0]), (72, [1, 0, 0, 0, 0, 0]), (86, [0, 0, 0, 0, 0, 1])] instead of [0 0 0 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 2000 samples of np ECG the prediction failed to detect a disease: 0 times\n",
      "It means the sensitivity is 100 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/1dAVB\\-------------------------------\n",
      "Over a number of 365 samples of 1dAVB ECG the prediction failed to detect a disease: 65 times\n",
      "It means the precision is 82.2 %\n",
      "Examples of errors: [(2011, [0, 0, 0, 0, 0, 0]), (2013, [0, 0, 0, 0, 0, 0]), (2014, [1, 0, 0, 1, 0, 0]), (2016, [0, 0, 0, 0, 0, 0]), (2021, [0, 0, 0, 0, 0, 0]), (2030, [0, 0, 0, 0, 0, 0]), (2039, [1, 0, 1, 0, 0, 0]), (2044, [0, 0, 0, 0, 0, 0]), (2048, [1, 0, 1, 0, 0, 0]), (2059, [0, 0, 0, 0, 1, 0])] instead of [1 0 0 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 365 samples of 1dAVB ECG the prediction failed to detect a disease: 49 times\n",
      "It means the sensitivity is 86.6 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/RBBB\\-------------------------------\n",
      "Over a number of 605 samples of RBBB ECG the prediction failed to detect a disease: 77 times\n",
      "It means the precision is 87.3 %\n",
      "Examples of errors: [(2367, [0, 0, 0, 1, 0, 0]), (2370, [0, 1, 0, 0, 1, 1]), (2372, [1, 1, 0, 1, 0, 0]), (2374, [0, 0, 0, 0, 0, 0]), (2380, [0, 1, 0, 0, 0, 1]), (2404, [1, 1, 0, 1, 0, 0]), (2413, [0, 0, 0, 0, 0, 0]), (2420, [1, 1, 0, 0, 0, 0]), (2442, [1, 1, 0, 0, 0, 0]), (2453, [1, 1, 0, 0, 0, 0])] instead of [0 1 0 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 605 samples of RBBB ECG the prediction failed to detect a disease: 22 times\n",
      "It means the sensitivity is 96.4 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/LBBB\\-------------------------------\n",
      "Over a number of 392 samples of LBBB ECG the prediction failed to detect a disease: 54 times\n",
      "It means the precision is 86.2 %\n",
      "Examples of errors: [(2969, [0, 0, 1, 0, 0, 0]), (2981, [0, 0, 0, 0, 0, 0]), (2987, [1, 0, 1, 0, 0, 0]), (2990, [0, 0, 1, 1, 0, 0]), (2995, [0, 0, 0, 0, 1, 0]), (3001, [1, 0, 1, 0, 0, 0]), (3019, [1, 0, 1, 0, 0, 0]), (3023, [0, 0, 0, 0, 0, 0]), (3045, [1, 0, 1, 0, 0, 0]), (3052, [1, 0, 1, 0, 0, 0])] instead of [0 0 1 0 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 392 samples of LBBB ECG the prediction failed to detect a disease: 19 times\n",
      "It means the sensitivity is 95.2 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/SB\\-------------------------------\n",
      "Over a number of 416 samples of SB ECG the prediction failed to detect a disease: 41 times\n",
      "It means the precision is 90.1 %\n",
      "Examples of errors: [(3372, [0, 1, 0, 1, 0, 0]), (3391, [1, 0, 0, 1, 0, 0]), (3393, [0, 0, 0, 0, 0, 0]), (3411, [0, 0, 0, 0, 0, 0]), (3437, [0, 0, 0, 0, 0, 0]), (3440, [0, 0, 0, 0, 0, 0]), (3444, [1, 0, 0, 1, 0, 0]), (3454, [0, 0, 0, 0, 0, 0]), (3463, [0, 0, 0, 0, 0, 0]), (3469, [0, 0, 0, 0, 0, 0])] instead of [0 0 0 1 0 0]\n",
      "--/AND\\--\n",
      "Over a number of 416 samples of SB ECG the prediction failed to detect a disease: 27 times\n",
      "It means the sensitivity is 93.5 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/AF\\-------------------------------\n",
      "Over a number of 466 samples of AF ECG the prediction failed to detect a disease: 71 times\n",
      "It means the precision is 84.8 %\n",
      "Examples of errors: [(3794, [0, 0, 0, 0, 0, 0]), (3812, [0, 0, 1, 0, 1, 0]), (3818, [0, 0, 0, 0, 0, 0]), (3838, [0, 0, 0, 0, 0, 0]), (3843, [0, 0, 1, 0, 1, 0]), (3849, [0, 0, 0, 0, 1, 1]), (3857, [1, 1, 0, 0, 0, 0]), (3859, [0, 0, 0, 0, 0, 1]), (3860, [0, 0, 0, 1, 0, 0]), (3864, [0, 0, 0, 0, 0, 0])] instead of [0 0 0 0 1 0]\n",
      "--/AND\\--\n",
      "Over a number of 466 samples of AF ECG the prediction failed to detect a disease: 53 times\n",
      "It means the sensitivity is 88.6 %\n",
      "------------------------------------------------------------------\n",
      "-------------------------------/ST\\-------------------------------\n",
      "Over a number of 610 samples of ST ECG the prediction failed to detect a disease: 26 times\n",
      "It means the precision is 95.7 %\n",
      "Examples of errors: [(4261, [0, 0, 0, 0, 1, 1]), (4293, [0, 0, 0, 0, 1, 1]), (4294, [0, 0, 0, 0, 0, 0]), (4309, [0, 0, 0, 0, 0, 0]), (4315, [1, 0, 0, 0, 0, 1]), (4341, [1, 0, 0, 0, 0, 1]), (4363, [0, 1, 0, 0, 0, 1]), (4372, [0, 1, 0, 0, 0, 1]), (4409, [1, 0, 0, 0, 0, 1]), (4477, [0, 0, 0, 0, 0, 0])] instead of [0 0 0 0 0 1]\n",
      "--/AND\\--\n",
      "Over a number of 610 samples of ST ECG the prediction failed to detect a disease: 11 times\n",
      "It means the sensitivity is 98.2 %\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------/TOTAL\\---------------------------------\")\n",
    "print(\"Over a number of 4900 samples the prediction failed:\",nb_fail,\"times\")\n",
    "print(\"It means the total precision is {0:.3g} %\".format((1-(nb_fail/(pos+1)))*100))\n",
    "print(\"--/AND\\--\")\n",
    "print(\"Over a number of 4900 samples the prediction failed to detect a disease:\",sensi,\"times\")\n",
    "print(\"It means the total sensitivity is {0:.3g} %\".format((1-(sensi/(pos+1)))*100))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "for ind,fails in enumerate(failed_prec):\n",
    "    print(\"-------------------------------/{}\\-------------------------------\".format(list(carac.keys())[ind]))\n",
    "    print(\"Over a number of {} samples of {} ECG the prediction failed to detect a disease: {} times\".format(lens[ind],list(carac.keys())[ind],len(fails)))\n",
    "    print(\"It means the precision is {0:.3g} %\".format((1-(len(fails)/lens[ind]))*100))\n",
    "    print('Examples of errors: {} instead of {}'.format([(i,y_score_conv[i]) for i in fails[:10]],y_true[lens_ok[ind]-1]))\n",
    "    print(\"--/AND\\--\")\n",
    "    print(\"Over a number of {} samples of {} ECG the prediction failed to detect a disease: {} times\".format(lens[ind],list(carac.keys())[ind],len(failed_sensi[ind])))\n",
    "    print(\"It means the sensitivity is {0:.3g} %\".format((1-(len(failed_sensi[ind])/lens[ind]))*100))\n",
    "    print(\"------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
