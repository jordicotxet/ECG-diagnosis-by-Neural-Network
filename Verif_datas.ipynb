{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf664b3-147f-4796-93bc-f6a8a4724442",
   "metadata": {},
   "source": [
    "# Verification of the diagnosis by the RIBEIRO PRE-TRAINED Model \n",
    "  \n",
    "- On this notebook we wiil look further into the stats of the RIBEIRO NN and confirm the results annonced by his team.  \n",
    "- We will work over a part of the giant dataset of more thant 2 millions 12-lead ECG.  \n",
    "- For that we take 30000 ECG that will construct our validation dataset. It represents aproximately 1.3% of the whole training dataset\n",
    "- We will create 7 datasets, one for each cardiac disease and the last one if the patient is healthy.  \n",
    "- Also, in order to understand why the prediction is not correct, a {disease}_prob dataset will be created and some of the ECG that causes problem to the machine will be printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ec975-2923-4289-bae7-cf03acd30e03",
   "metadata": {},
   "source": [
    "# I. Determine diseases datasets  \n",
    "  \n",
    "First we analyse the 1.5%_dataset to find what are the index of the abnormal-ECG, we categorise them into 7 lists.  \n",
    "Those lists will be limited to 2 thousands index, it means that our stats won't be perfect but they will approximate the real ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68730148-0341-43ed-852c-f8b9f596d1b2",
   "metadata": {},
   "source": [
    "  ### I.1. Open datas and take only 30000 samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd656b-4ec1-4752-b434-90647dc0b34e",
   "metadata": {},
   "source": [
    "Taking the annotated ECG in order to compare the prediction to the annotation at the end.  \n",
    "We want 1 or 2 thousands ECG of each diseases, then 30000 datas looks alright to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1752fd7c-6cca-4c12-8850-5ec307336843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40635it [00:07, 5340.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du dataset est de : (30000, 4096, 12)\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_SHAPE = 30000 \n",
    "PATH_TO_GIANT = '/media/cotxetjordi/TOSHIBA EXT/Data_2/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + \"preprocessed_traces15pc.hdf5\", \"r\") as f:\n",
    "    i = 0\n",
    "    signals = []\n",
    "    id_exams = []\n",
    "    ids = list(f[\"id_exam\"][:60000])  #Only 40000 raw data needed\n",
    "\n",
    "    for index,data in tqdm(enumerate(f[\"signal\"])):\n",
    "\n",
    "        id = ids[index]\n",
    "\n",
    "        if id < 2200000: # Only takes exams with annotations available in annotations.csv\n",
    "            \n",
    "            signals.append(data)\n",
    "            id_exams.append(id)\n",
    "            i+=1\n",
    "            \n",
    "        if i == TEST_SHAPE:\n",
    "            break\n",
    "            \n",
    "        \n",
    "signals = np.array(signals)\n",
    "id_exam = np.array(id_exams)\n",
    "print(\"La taille du dataset est de :\", np.shape(signals))\n",
    "print(len(id_exams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e9832-9922-4336-b51b-3bf9ac8beed7",
   "metadata": {},
   "source": [
    "### I.2.a) Save the datas to use it later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed7a879-ba75-46c0-a24f-a10d98f94da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'a') as new_file:\n",
    "    id_exam_data = new_file.create_dataset(name = \"id_exam\",data = id_exams)\n",
    "    signals_data = new_file.create_dataset(name = \"signal\",data= = signals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1de8e3-3b86-424e-86ff-05a308af8bcb",
   "metadata": {},
   "source": [
    "### I.2.b) Load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f746a964-5d55-47fe-bae9-187a30300011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 4096, 12)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "PATH_TO_GIANT = '/media/cotxetjordi/TOSHIBA EXT/Data_2/'\n",
    "\n",
    "with h5py.File(PATH_TO_GIANT + 'datas_to_verif.hdf5', 'r') as new_file:\n",
    "    id_exams = new_file[\"id_exam\"]\n",
    "    X = new_file[\"signal\"][()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f23d20-e7b5-4ed4-88e0-007daefee600",
   "metadata": {},
   "source": [
    "## I.3. Create 7 datasets, one for each disease\n",
    "### 3.1. Find the annotation of each signal of our dataset and divide it into 7 datasets \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62dd2de8-1d86-40b2-a081-c5b3efe45d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962\n",
      "--- 159.48490262031555 seconds ---\n",
      "1864\n",
      "--- 317.4650228023529 seconds ---\n",
      "2693\n",
      "--- 475.2715952396393 seconds ---\n",
      "3552\n",
      "--- 631.7928097248077 seconds ---\n",
      "4381\n",
      "--- 787.9370865821838 seconds ---\n",
      "5318\n",
      "--- 943.591845035553 seconds ---\n",
      "6253\n",
      "--- 1099.3485352993011 seconds ---\n",
      "7151\n",
      "--- 1254.929848432541 seconds ---\n",
      "8021\n",
      "--- 1410.2463309764862 seconds ---\n",
      "8893\n",
      "--- 1564.9671349525452 seconds ---\n",
      "9839\n",
      "--- 1718.799234867096 seconds ---\n",
      "10720\n",
      "--- 1873.2899703979492 seconds ---\n",
      "11640\n",
      "--- 2027.871402978897 seconds ---\n",
      "--- 2086.9377748966217 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "PATH_TO = '/media/cotxetjordi/TOSHIBA EXT/Data_2/' #Path to your csv file \n",
    "counter = -1\n",
    "start_time = time.time()\n",
    "with open(PATH_TO + 'annotations_clean.csv','r', newline='') as csvfile:\n",
    "        COUNTER = 0\n",
    "        i = 1\n",
    "        carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "        f = csv.reader(csvfile)\n",
    "        indexes = list(carac.keys())\n",
    "        \n",
    "        \n",
    "        for line in f:\n",
    "\n",
    "            if i in id_exams:\n",
    "                COUNTER +=1\n",
    "                if \"1\" in line and line.count(\"1\") == 1 : #Just taking the \"mono-disease\" ECG to analyse\n",
    "                    disease_ids = carac[indexes[line.index(\"1\")+1]]\n",
    "                    if len(disease_ids) < 2000:\n",
    "                        disease_ids.append(id_exams.index(i))\n",
    "\n",
    "                elif len(carac[\"np\"]) < 2000:\n",
    "                    carac[\"np\"].append(id_exams.index(i))\n",
    "                    \n",
    "            if i % 50000 == 0:\n",
    "                    print(COUNTER)\n",
    "                    print(\"--- %s seconds ---\" % (time.time() - start_time))            \n",
    "            if COUNTER == 12000:\n",
    "                break\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "           \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0344133-1c86-440e-80a3-b45b86a27c78",
   "metadata": {},
   "source": [
    "### I.3.a) Save the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00310adb-2743-4623-9273-797bc2e4adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np\n",
      "1dAVB\n",
      "RBBB\n",
      "LBBB\n",
      "SB\n",
      "AF\n",
      "ST\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for index, (dis,ids) in enumerate(carac.items()) :\n",
    "\n",
    "    np.save(dis,ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50c1ae-c4fa-4a0d-a7f3-93268cd64a0b",
   "metadata": {},
   "source": [
    "### I.3.b) Load the 7 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18ec24c7-44bf-4a23-9b7e-4af7b3e2fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "np has 2000 samples\n",
      "(135,)\n",
      "1dAVB has 135 samples\n",
      "(272,)\n",
      "RBBB has 272 samples\n",
      "(140,)\n",
      "LBBB has 140 samples\n",
      "(164,)\n",
      "SB has 164 samples\n",
      "(189,)\n",
      "AF has 189 samples\n",
      "(282,)\n",
      "ST has 282 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "carac = {\"np\":[],\"1dAVB\":[],\"RBBB\":[],\"LBBB\":[],\"SB\":[],\"AF\":[],\"ST\":[]}\n",
    "for index,(key,ids) in enumerate(carac.items()):\n",
    "    \n",
    "    carac[key] = np.load(key+ \".npy\")\n",
    "\n",
    "    print(key,\"has\", len(carac[key]),\"samples\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e353d-d821-4cf4-939c-8bdedf0a3984",
   "metadata": {},
   "source": [
    "### I.4 Create an array composed by 7 arrays of disease-acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a0d0bda-a64f-4267-af18-7c831b8c6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4096, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-fcfa0ef8bd8e>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_all = np.array([[X[index] for index in ids] for ids in carac.values()]) #don't forget to converte each of the 7 lists when you will predict using the pre-trained model\n"
     ]
    }
   ],
   "source": [
    "X_test_all = np.array([[X[index] for index in ids] for ids in carac.values()]) #don't forget to converte each of the 7 lists when you will predict using the pre-trained model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968aa5a5-9815-4144-bdcd-1710f8ece052",
   "metadata": {},
   "source": [
    "## II. Prediction test\n",
    "### II.1 Load and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51a860e4-9581-4fb4-9b0b-e6028686b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cotxetjordi/Documents/Stage_Notebook/Rendu\n"
     ]
    }
   ],
   "source": [
    "cd Documents/Stage_Notebook/Rendu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e65c7d2-8d69-4eb3-b6d1-356f73241537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Import packages\n",
    "import tensorflow.compat.v1 as tf  #Fait appel à la binliothèque Tensorflow Version 1.xx\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import warnings\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "\n",
    "PATH_TO_MODEL  = 'model.hdf5'\n",
    "\n",
    "\n",
    "\n",
    "# Import model\n",
    "model = keras.models.load_model(PATH_TO_MODEL,compile=False)\n",
    "\n",
    "#model = load_model(PATH_TO_MODEL, compile=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc950eb-5b63-4deb-90d6-67a207bb6041",
   "metadata": {},
   "source": [
    "### II.2. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40da69dc-86a3-4e97-a404-e40c07e8ce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.1088882e-07 7.6678202e-09 7.9162641e-09 5.4571192e-10 3.7799751e-08\n",
      "  4.1579469e-09]]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "prediction = model.predict(np.array([X_test_all[2][1]]))\n",
    "for X in X_test_all:\n",
    "    prediction = model.predict(np.array([X[0]]),verbose = 1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d20b1-da47-442a-a7bf-a1d78ae15af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
